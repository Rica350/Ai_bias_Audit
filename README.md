# Bias Audit Report: Hiring Dataset  
_A machine learning fairness audit with bias detection, mitigation, and visualization_  

## ğŸ“Œ Overview  
This project conducts a **bias audit on a hiring dataset** to analyze gender-related bias in hiring decisions. Using machine learning and fairness frameworks, the project evaluates **fairness metrics**, applies **bias mitigation techniques**, and visualizes results for stakeholder communication.  

The goal is to ensure **responsible AI adoption** in recruitment by detecting and addressing discriminatory patterns in automated decision-making systems.  

---

## âœ¨ Features  
- Preprocessing of hiring dataset  
- Training **baseline Logistic Regression model**  
- Computation of **fairness metrics** (Accuracy, Selection Rate, TPR, FPR)  
- Bias **mitigation using reweighting techniques**  
- **Visualization** with confusion matrices & fairness metric plots  
- Concise **summary table** for stakeholder reporting  
- Ready-to-use **Google Colab Notebook** (no setup required)  
- **Stakeholder-friendly PowerPoint presentation** included  

---

## ğŸ“Š Dataset  
- Dataset includes hiring decision records with demographic information.  
- **Target variable**: Hiring decision (Hired / Not Hired).  
- **Sensitive attribute**: Gender.  
- Dataset is split into **train** and **test sets**.  

---

## ğŸ› ï¸ Technologies & Tools  
This project was implemented in **Python (Jupyter Notebook)** using the following key libraries:  

- **Data Handling**: `pandas`, `numpy`  
- **Machine Learning**: `scikit-learn`  
- **Fairness Framework**: [`fairlearn`](https://fairlearn.org/)  
- **Visualization**: `matplotlib`, `seaborn`  
- **Reporting**: `pandas` summary tables  

---

## ğŸ“‚ Project Structure  
